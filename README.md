# ğŸ§¬ Transformers_for_Peptide_Classification

This repository contains a **multi-modal transformer architecture** designed to process **time series**, **image**, and **feature vector** data simultaneously.  
It can be trained in a **masked autoencoder (MAE)** style to reconstruct inputs and later fine-tuned on a **classification task**.

The model was developed to **classify peptides and proteins from nanopore measurements**.  
To that end, time series signals are:
- Transformed into **Wavelet images**
- Enriched with **catch22 features**
- Combined with the **original time series** data

This approach leads to a **significant improvement in peptide classification** accuracy.  
The architecture is flexible and can be adapted for other **multi-modal classification tasks** with arbitrary combinations of time series, images, and feature vectors.

---

> âš ï¸ **Patent Notice**  
> The described method is protected under **German Patent No. 10 2025 123 407.8**.  
> A related **scientific publication** is currently in preparation.  
> Use of this patented approach for commercial or derivative work may require a **license agreement**.

---

## ğŸ“š Previous Work

This model builds on prior research conducted at the Institute for Computational Physics at the University of Stuttgart, which laid the foundation for the multi-modal transformer approach:

1. **HoÃŸbach et al., 2025** - *Peptide classification from statistical analysis of nanopore sensing experiments*
   DOI: [10.1063/5.0250399](https://doi.org/10.1063/5.0250399)
2. **Tovey et al., 2025** - *Deep Learning-Driven Peptide Classification in Biological Nanopores*
   DOI: [10.48550/arXiv.2509.14029](https://doi.org/10.48550/arXiv.2509.14029)
   
---

## âœ¨ Features

- ğŸ”— **Multi-Modal Transformer** capable of processing any number of **time series**, **image**, and **feature vector** inputs  
- ğŸ§  **Pre-training as a Masked Autoencoder (MAE)**  
- ğŸ¯ **Fine-tuning for classification**  
- âš–ï¸ **Flexible configuration** â€” train and compare different model subsets:
  - ğŸ¤ Multi-modal model combining **time series** and **images**
  - ğŸ§® Multi-modal model combining **time series** and a **feature vector**
  - ğŸ–¼ï¸ **Vision Transformer (ViT)** implementation (MAE pre-training + classification)
  - ğŸ“ˆ **Time Series Transformer** implementation (MAE pre-training + classification)
- ğŸ” **Attention weight extraction** for interpretability and attention map analysis

## âš™ï¸ Installation

### Requirements

### Setup

## ğŸš€ Usage

## Architecture

![Model Architecture](images/Multi-Modal-Architecture.png)



